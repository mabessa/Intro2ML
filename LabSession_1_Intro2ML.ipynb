{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WS2332 - Project 7 - Lecture 2\n",
    "Miguel Bessa\n",
    "<div>\n",
    "<img src=docs/tudelft_logo.jpg width=300px></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** Lab Session 1 of course WS2332 (Project 7): Introduction to Machine Learning\n",
    "\n",
    "* Today's lecture focuses on **regression via supervised learning in 1D**\n",
    "\n",
    "**How:** Jointly workout this notebook\n",
    "* GitHub: https://github.com/mabessa/Intro2ML\n",
    "    1. You can do this locally in your computer (but you have to have the Python packages installed):\n",
    "        * clone the repository to your computer: git clone https://github.com/mabessa/Intro2ML\n",
    "        * load jupyter notebook (it will open in your internet browser): jupyter notebook\n",
    "        * search for this notebook in your computer and open it\n",
    "    2. Or you can use Google's Colab (no installation required, but times out if idle):\n",
    "        * go to https://colab.research.google.com\n",
    "        * login\n",
    "        * File > Open notebook\n",
    "        * click on Github (no need to login or authorize anything)\n",
    "        * paste the git link: https://github.com/mabessa/Intro2ML\n",
    "        * click search and then click on the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple tutorial is based on a script I created for this article: https://imechanica.org/node/23957\n",
    "\n",
    "It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen.\n",
    "\n",
    "License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start by importing basic modules into Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment\n",
    "import numpy as np # fundamental scientific computing module\n",
    "import matplotlib.pyplot as plt # plotting module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline for today\n",
    "\n",
    "1. Quick introduction to Python\n",
    "2. Polynomial approximations (global & noiseless)\n",
    "3. Polynomial approximations for noisy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick introduction to Python\n",
    "\n",
    "Let's plot the function $x\\sin(x)$ in the domain $x\\in[0,10]$.\n",
    "\n",
    "1. Define the function $f(x) = x\\sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a vector of 50 points that are uniformly spaced between 0 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50 # number of points for plotting the function\n",
    "x_data = np.linspace(0, 10, n_data) # uniformly spaced points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.20408163  0.40816327  0.6122449   0.81632653  1.02040816\n",
      "  1.2244898   1.42857143  1.63265306  1.83673469  2.04081633  2.24489796\n",
      "  2.44897959  2.65306122  2.85714286  3.06122449  3.26530612  3.46938776\n",
      "  3.67346939  3.87755102  4.08163265  4.28571429  4.48979592  4.69387755\n",
      "  4.89795918  5.10204082  5.30612245  5.51020408  5.71428571  5.91836735\n",
      "  6.12244898  6.32653061  6.53061224  6.73469388  6.93877551  7.14285714\n",
      "  7.34693878  7.55102041  7.75510204  7.95918367  8.16326531  8.36734694\n",
      "  8.57142857  8.7755102   8.97959184  9.18367347  9.3877551   9.59183673\n",
      "  9.79591837 10.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's calculate the values of $f(x)=x \\sin(x)$ for each of the 50 points of vector x_data\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Compute f(x) for the points we created and save these values in y_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "\n",
    "# Write your code here:\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now let's plot the function from the 50 points we know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-569cd4043851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot points and interpolate them:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m ax1.plot(x_data, y_data, 'ro:', markersize=6, linewidth=2,\n\u001b[0m\u001b[1;32m      5\u001b[0m          label=u'ground truth: $f(x) = x\\,\\sin(x)$')\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_data' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots() # This opens a new figure\n",
    "\n",
    "# Plot points and interpolate them:\n",
    "ax1.plot(x_data, y_data, 'ro:', markersize=6, linewidth=2,\n",
    "         label=u'ground truth: $f(x) = x\\,\\sin(x)$')\n",
    "\n",
    "ax1.set_xlabel('$x$') # label of the x axis\n",
    "ax1.set_ylabel('$f(x)$') # label of the y axis\n",
    "ax1.legend(loc='upper left') # plot legend in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if we have lots of points, even linear interpolation between the points can approximate the function very well.\n",
    "\n",
    "However, what if we use just a few points from our dataset x_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5 # points to train the algorithm\n",
    "x_train = np.linspace(0, 10, n_train) # 5 points uniformly distributed\n",
    "y_train = f(x_train)\n",
    "\n",
    "ax1.plot(x_train, y_train, 'k*', markersize=12,\n",
    "         label=\"Training points\") # Markers locating training points\n",
    "\n",
    "ax1.plot(x_train, y_train, 'g-', linewidth=2,\n",
    "         label=u'local linear interpolation') # linear interpolation\n",
    "                                              # plotted\n",
    "\n",
    "ax1.legend(loc='upper left') # replot legend\n",
    "fig1 # replot fig1 now overlaying the plot in the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called local interpolation because each line only depends on the two points it is connecting (not on the other points).\n",
    "\n",
    "* Next, we are going to look into global interpolation with polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Polynomial approximations (global & noiseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the polynomial predictor from scikit-learn\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial fit\n",
    "from sklearn.linear_model import LinearRegression # For Least Squares\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),\n",
    "                           LinearRegression())\n",
    "\n",
    "poly_model.fit(x_train,y_train) # fit the polynomial to our 5 points\n",
    "                                # in x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But that gives an error!**\n",
    "\n",
    "Scikit-learn models expect the input, i.e. x_train, to be a 2D array (a matrix) where each line corresponds to one point and each column corresponds to a **FEATURE** of that point.\n",
    "\n",
    "So, instead of expecting an array like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It expects this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(x_train, (-1, 1)) # convert vector to 2d array\n",
    "\n",
    "# Other way of doing this would be:\n",
    "# X_train = x_train[:, np.newaxis]\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, each point has only one feature: the $x$ coordinate of that point.\n",
    "\n",
    "* Let's try again to fit the polynomial to the data, but now using X_train instead of x_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.fit(X_train,y_train) # fit the polynomial to our 5 points\n",
    "                                # in X_train which is a 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!\n",
    "\n",
    "This means that scikit-learn has fit the polynomial model to our 5 training points.\n",
    "\n",
    "We can then use this model to predict all 50 points we defined in x_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In scikit-learn, predicting from a model is a one-liner:\n",
    "y_pred = poly_model.predict(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Something went wrong again**. What happened?\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Use our polynomial model to predict all 50 points we defined in x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "\n",
    "# Write your code here:\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got it right by plotting the polynomial prediction on top of fig1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x_data and prediction as a blue line:\n",
    "ax1.plot(x_data, y_pred, 'b-', linewidth=2,\n",
    "         label=\"Polynomial of degree %d prediction\" % degree)\n",
    "\n",
    "# Replot figure and legend:\n",
    "ax1.legend(loc='upper left')\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.\n",
    "\n",
    "Yet, our polynomial (blue) is clearly different to the function we want to \"learn\", i.e. $x \\sin(x)$.\n",
    "\n",
    "How do we evaluate the quality of our approximation?\n",
    "\n",
    "* By evaluating the error of our polynomial model in the points that we didn't use in the fit.\n",
    "\n",
    "Two common metrics are $R^2$ and $MSE$ (you will have to search for them and explain them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import error metrics:\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model we fitted\n",
    "mse_value = mean_squared_error(y_data, y_pred)\n",
    "r2_value = r2_score(y_data, y_pred)\n",
    "\n",
    "print('MSE for polynomial = ', mse_value)\n",
    "print('R2 score for polynomial = ', r2_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these predictions are not great because:\n",
    "\n",
    "* We want $MSE$ to be as low as possible\n",
    "\n",
    "* The closer $R^2$ is to 1.0 the better\n",
    "\n",
    "You will dive deeper in this when solving the Lab Assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial approximations for noisy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a different case now.\n",
    "\n",
    "Imagine that your data is imperfect.\n",
    "\n",
    "This is very common in practice because usually data comes from experimental measurements.\n",
    "\n",
    "For comparison purposes, let's \"fabricate\" such dataset based on the dataset we considered in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1987 # set a random seed so that everyone gets the same result\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Let's perturb every y_data point with Gaussian noise\n",
    "random_std = 0.5 + 1.0 * np.random.random(y_data.shape)\n",
    "\n",
    "# Then, take the random value for STD from 0.5 to 1.5 for each\n",
    "# data point and create noise following a Gaussian distribution with\n",
    "# that STD at that point:\n",
    "noise = np.random.normal(0, random_std)\n",
    "\n",
    "# The perturbed data becomes:\n",
    "y_noisy_data = y_data + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we plot the noisy data with the noiseless function that we would like to discover $x \\sin(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots() # This opens a new figure\n",
    "\n",
    "# Plot the noiseless function (\"the ground thruth\")\n",
    "ax2.plot(x_data, y_data, 'r:', linewidth=2,\n",
    "         label=u'ground truth: $f(x) = x\\,\\sin(x)$')\n",
    "\n",
    "# Plot the noisy dataset that we are given:\n",
    "plt.errorbar(x_data, y_noisy_data, random_std, fmt='kx',\n",
    "             markersize=6, label=u'noisy dataset')\n",
    "\n",
    "ax2.set_xlabel('$x$') # label of the x axis\n",
    "ax2.set_ylabel('$f(x)$') # label of the y axis\n",
    "ax2.legend(loc='upper left') # plot legend in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a couple of things:\n",
    "\n",
    "* The black \"x\" marks the average value if we were to measure many times the same property.\n",
    "\n",
    "* The black bars indicate the noise in each data point (each data point has a different noise value). Formally, we call this aleatoric uncertainty because if we were to measure many times the output for a given input we would obtain that average and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 1 we decided to have two different datasets: one with 50 points and the other with 5 new points (not within the original 50).\n",
    "\n",
    "This does not reflect usual practice.\n",
    "\n",
    "In data science we are usually given a dataset, and then we need to train and test our algorithms with the **same** dataset.\n",
    "\n",
    "However, to test the algorithm we have to use data that we have not used in training, otherwise we would be cheating!\n",
    "\n",
    "This is done by splitting the dataset (in this case x_data) into two sets:\n",
    "\n",
    "1. **Training** set (for example: 75% of the dataset)\n",
    "\n",
    "\n",
    "2. **Test** set with the remaining points of the dataset\n",
    "\n",
    "Scikit-learn has a very easy way of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_data = np.reshape(x_data,(-1,1)) # a 2D array that scikit-learn likes\n",
    "\n",
    "# Let's split the data points into 10% for the training set (5 points)\n",
    "# and the rest for the test set:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                    y_noisy_data, test_size=0.90,\n",
    "                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the train_test_split module of scikit-learn picks points pseudo-randomly according to the random_state seed value.\n",
    "\n",
    "Let's visualize the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can convert back to 1D array for plotting.\n",
    "x_train = X_train.ravel() # THIS IS NOT NECESSARY, PYTHON CAN ALSO PLOT from the 2D array\n",
    "x_test = X_test.ravel() # THIS IS NOT NECESSARY, PYTHON CAN ALSO PLOT from the 2D array\n",
    "\n",
    "# Plot the noisy training dataset:\n",
    "ax2.plot(x_train, y_train, 'g*', markersize=18,\n",
    "         label=\"Training points\") # Markers locating training points\n",
    "\n",
    "ax2.plot(x_test, y_test, 'bs', markersize=6,\n",
    "         label=\"Testing points\") # Markers locating training points\n",
    "\n",
    "ax2.set_xlabel('$x$') # label of the x axis\n",
    "ax2.set_ylabel('$f(x)$') # label of the y axis\n",
    "ax2.legend(loc='upper left') # plot legend in the upper left corner\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new figure with less clutter by just plotting the ground truth function and the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots() # This opens a new figure\n",
    "\n",
    "# Plot the noiseless function (\"the ground thruth\")\n",
    "ax3.plot(x_data, y_data, 'r:', linewidth=2,\n",
    "         label=u'ground truth: $f(x) = x\\,\\sin(x)$')\n",
    "\n",
    "ax3.plot(x_train, y_train, 'g*', markersize=18,\n",
    "         label=\"Training points\") # Markers locating training points\n",
    "\n",
    "ax3.set_xlabel('$x$') # label of the x axis\n",
    "ax3.set_ylabel('$f(x)$') # label of the y axis\n",
    "ax3.legend(loc='upper left') # plot legend in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fit a polynomial of degree 4 to this training data and calculate the $R^2$ and $MSE$ metrics for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Write your code here:\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done...\n",
    "\n",
    "Yet, this does not seem like a great result, does it?\n",
    "\n",
    "The $R^2$ value is so bad that it is even negative!\n",
    "\n",
    "* What explains this result?\n",
    "\n",
    "* Can we do something to fix this while still using polynomials?\n",
    "\n",
    "* If we used more points would that help?\n",
    "\n",
    "* What if we increased the degree of the polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will explore these and other things in Part 1 of Lab Assignment...\n",
    "\n",
    "Have fun!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv3",
   "language": "python",
   "name": "mlenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
