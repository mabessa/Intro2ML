{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=docs/tudelft_logo.jpg width=50%>\n",
    "\n",
    "## LIGHTEN Project: Training School #2\n",
    "\n",
    "## Practical Session 1: Introduction to Gaussian Processes and Artificial Neural Networks\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: M.A.Bessa@tudelft.nl\">M.A.Bessa@tudelft.nl</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A simple tutorial on Supervised Regression for the Training School of the EU project LIGHTEN\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/mabessa/Intro2ML/LIGHTEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**References/resources to create this notebook:**\n",
    "* This simple tutorial is still based on a script I created for this article: https://imechanica.org/node/23957\n",
    "* It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen.\n",
    "* [Black box figure](https://openclipart.org/download/218057/black_boxes.svg)\n",
    "* For artificial neural networks, using materials from Andreas Mueller (Lecture 22): https://github.com/amueller/COMS4995-s19\n",
    "\n",
    "Apologies in advance if I missed some reference used in this notebook. Please contact me if that is the case, and I will gladly include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Install miniconda3 [here](https://docs.conda.io/en/latest/miniconda.html)\n",
    "2. Open a command window and create a virtual environment called \"3dasm\":\n",
    "```\n",
    "conda create -n 3dasm python=3 numpy scipy jupyter nb_conda matplotlib pandas scikit-learn rise tensorflow -c conda-forge\n",
    "```\n",
    "3. Install [git](https://github.com/git-guides/install-git), open command window & clone the repository to your computer:\n",
    "```\n",
    "git clone https://github.com/mabessa/Intro2ML\n",
    "```\n",
    "4. Load jupyter notebook by typing in (anaconda) command window (it will open in your internet browser):\n",
    "```\n",
    "conda activate 3dasm\n",
    "jupyter notebook\n",
    "```\n",
    "5. Open notebook (Intro2ML/LIGHTEN/TS2_Session1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/mabessa/Intro2ML\n",
    "6. click search and then click on the notebook for this Lecture (Intro2ML/LIGHTEN/TS2_Session1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plotting tools needed in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "import numpy as np # import numpy to handle a lot of things!\n",
    "from IPython.display import display, Math # to print with Latex math\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\" # render higher resolution images in the notebook\n",
    "plt.style.use(\"seaborn\") # style for plotting that comes from seaborn\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4) # rescale figure size appropriately for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Introduction tutorial on Gaussian Processes (no theory today!)\n",
    "    - Using Scikit-learn for Gaussian Process Regression (noiseless and noisy datasets)\n",
    "* Introduction tutorial on Artificial Neural Networks (no theory today!)\n",
    "    - Using Keras for regression with Artificial Neural Networks (noiseless and noisy datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal is for you to be able to use these models as *black boxes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This session focuses on how to train:\n",
    "    - **Gaussian Processes** using [scikit-learn](https://scikit-learn.org)\n",
    "    - **Artificial Neural Networks** (ANNs) using [keras](https://keras.io/) and [tensorflow](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is it a good idea to use machine learning models without understanding them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* No.\n",
    "\n",
    "But I am not talented enough to teach Machine Learning in 1.5 hours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, this session is like a *teaser*. Hopefully it motivates you to do a Machine Learning course!\n",
    "\n",
    "* The course I teach about machine learning is in the following repo: https://github.com/bessagroup/3dasm_course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, let's consider supervised Machine Learning (ML) models for regression as black boxes.\n",
    "\n",
    "<img src=\"docs/black_box.png\" title=\"Machine learning as a black box\" width=\"50%\">\n",
    "\n",
    "* Each ML model has its own *expressivity* that depends on a set of parameters called **hyperparameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hyperparameters are parameters that you define before you start training the model using data (observations for your problem of interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If you understand the ML model you are using, then its hyperparameters can be meaningful to you. In that case, you *might* be able to choose reasonable hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Short tutorial on 1D regression with Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Processes are a simple to use but very powerful ML model.\n",
    "\n",
    "They are simple because they only require the definition of two hyperparaters:\n",
    "\n",
    "1. The **kernel function** $k(x_i,x_j)$, which can be many different kinds of functions (with some special properties).\n",
    "\n",
    "\n",
    "2. And the noise level $\\sigma_i^2$ which represents uncertainty (variance) of your data at each output point $y_i$. If the function you want to approximate is noiseless, i.e. if the measurements are exact, then you can use a small value for this parameter (e.g. $1\\times 10^{-10}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The kernel function is important because it assigns specific properties to your approximation of the data.\n",
    "\n",
    "* An example of a kernel function is the RBF: $k(x_i,x_j) = {\\color{red}\\eta}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}\\lambda}^2}\\right)}$\n",
    "\n",
    "\n",
    "* Every kernel function has a set of unknown parameters (in red) that will be learned by **training on the data**.\n",
    "    * For example, the RBF kernel has 2 parameters.\n",
    "    \n",
    "Here's two resources that are very useful to understand the role of the kernel function:\n",
    "\n",
    "* Kernel cookbook: https://www.cs.toronto.edu/~duvenaud/cookbook/\n",
    "* Visualizing different kernel functions: https://distill.pub/2019/visual-exploration-gaussian-processes/#MultipleKernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Viewing a Gaussian Process as a black box\n",
    "\n",
    "<img src=\"docs/black_box.png\" title=\"Machine learning as a black box\" width=\"50%\">\n",
    "\n",
    "* Where the hyperparameters for the Gaussian Process method are:\n",
    "\n",
    "    * Kernel function, for example the RBF: $k(x_i,x_j) = {\\color{red}\\eta}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}\\lambda}^2}\\right)}$\n",
    "    \n",
    "    * Noise at each data point: $\\sigma_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The <span style=\"color:red\">**key concept**</span> is that the prediction of the mean and variance of a new point $y^*$ depends on the values of the parameters of the kernel function (which are **UNKNOWN**).\n",
    "\n",
    "\n",
    "* However, despite the fact that we don't know the parameters of the kernel function, they can be obtained by **Bayesian inference**.\n",
    "\n",
    "Bayesian inference is possible by using Bayes rule to find the **posterior** information. This involves doing Marginalization and Conditioning.\n",
    "\n",
    "Here we don't cover this. Instead, we just let the code do it for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example for Gaussian Process regression of one-dimensional datasets.\n",
    "\n",
    "Let's start with a **noiseless** case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to \"learn\"\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "n_data = 50 # number of points in our dataset\n",
    "testset_ratio = 0.90 # ratio of test set points from the dataset\n",
    "x_data = np.linspace(0, 10, n_data) # uniformly spaced points\n",
    "y_data = f(x_data) # function values at x_data\n",
    "\n",
    "X_data = np.reshape(x_data,(-1,1)) # a 2D array that scikit-learn likes\n",
    "\n",
    "seed = 1987 # set a random seed so that everyone gets the same result\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Let's split into 10% training points and the rest for testing:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                    y_data, test_size=testset_ratio,\n",
    "                                    random_state=seed)\n",
    "\n",
    "x_train = X_train.ravel() # just for plotting later\n",
    "x_test = X_test.ravel() # just for plotting later\n",
    "\n",
    "print(\"Here's a print of X_train:\\n\", X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Process Regression (GPR) for noiseless datasets\n",
    "\n",
    "Let's use the RBF kernel for our predictions:\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}\\eta}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}\\lambda}^2}\\right)}\n",
    "$$\n",
    "\n",
    "with an initial guess for the parameters as: $\\eta = 1$ and $\\lambda = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ExpSineSquared, ConstantKernel\n",
    "\n",
    "# Define the kernel function\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) # This is the standard RBF kernel\n",
    "#kernel = 1.0 * RBF(10, (1e-2, 1e2)) # Same kernel as above (scikit-learn assumes constant variance if you just\n",
    "                                     # write RBF without the constant kernel or without multiplying by 1.0)\n",
    "\n",
    "# Other examples of kernels:\n",
    "#kernel = ExpSineSquared(length_scale=3.0, periodicity=3.14,\n",
    "#                       length_scale_bounds=(0.1, 10.0),\n",
    "#                       periodicity_bounds=(0.1, 10)) * RBF(3.0, (1e-2, 1e2))\n",
    "#kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=1.5)\n",
    "                \n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=20) # using a small alpha\n",
    "\n",
    "# Fit to data to determine parameters\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make the prediction on the entire dataset (for plotting)\n",
    "y_data_pred, sigma_data = gp_model.predict(X_data, return_std=True) # also output the uncertainty (std)\n",
    "\n",
    "# Predict for test set (for error metric)\n",
    "y_pred, sigma = gp_model.predict(X_test, return_std=True) # also output the uncertainty (std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(x_data, y_data, 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # function to learn\n",
    "\n",
    "ax1.plot(x_data, y_data_pred, 'b-', label=\"GPR prediction\")\n",
    "ax1.fill(np.concatenate([x_data, x_data[::-1]]),\n",
    "         np.concatenate([y_data_pred - 1.9600 * sigma_data,\n",
    "                        (y_data_pred + 1.9600 * sigma_data)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "\n",
    "ax1.plot(x_train, y_train, 'ro', markersize=6, label=\"training points\") # noiseless data\n",
    "ax1.plot(x_test, y_test, 'kX', markersize=6, label=\"testing points\") # Plot test points\n",
    "\n",
    "ax1.set_xlabel('$x$', fontsize=20)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax1.set_title(\"Posterior kernel: %s\"\n",
    "              % gp_model.kernel_, fontsize=20) # Show in the title the value of the hyperparameters\n",
    "ax1.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax1.legend(loc='upper left', fontsize=15)\n",
    "fig1.set_size_inches(8,8)\n",
    "plt.close(fig1) # close the plot to see it in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1 # plot figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1: Compare with the approximation of a polynomial of degree 4\n",
    "\n",
    "Let's fit a polynomial of degree 4 and compute the error metrics for that model as well as the above mentioned Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing the polynomial predictor from scikit-learn\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial fit\n",
    "from sklearn.linear_model import LinearRegression # For Least Squares\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "poly_model.fit(X_train,y_train) # fit the polynomial to our 5 points in X_train which is a 2D array  \n",
    "y_poly_pred = poly_model.predict(X_test) # prediction of our polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the function and the polynomial prediction\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "ax2.plot(x_data, y_data, 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # function to learn\n",
    "\n",
    "y_poly_data_pred = poly_model.predict(X_data) # prediction of our polynomial\n",
    "ax2.plot(x_data, y_poly_data_pred, 'b-', label=\"Polynomial prediction\")\n",
    "\n",
    "ax2.plot(x_train, y_train, 'ro', markersize=6, label=\"training points\") # noiseless data\n",
    "ax2.plot(x_test, y_test, 'kX', markersize=6, label=\"testing points\") # Plot test points\n",
    "\n",
    "ax2.set_xlabel('$x$', fontsize=20)\n",
    "ax2.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax2.set_title(\"Polynomial approximation\", fontsize=20)\n",
    "ax2.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax2.legend(loc='upper left', fontsize=15)\n",
    "fig2.set_size_inches(8,8)\n",
    "plt.close(fig2) # close the plot to see it in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Process Regression clearly approximates the function much better! And predicts the uncertainty!\n",
    "\n",
    "In practice, assessing the quality of approximations should be done with **error metrics**.\n",
    "\n",
    "* Scikit-learn has very useful error metrics for regression problems!\n",
    "    * The most common ones are the **mean squared error** ($\\text{MSE}$) and the **R-squared** error ($R^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score # Import error metrics\n",
    "# Compute MSE and R2 for the GP model\n",
    "gp_mse_value = mean_squared_error(y_test, y_pred)\n",
    "gp_r2_value = r2_score(y_test, y_pred)\n",
    "print('MSE for GPR = ', gp_mse_value)\n",
    "print('R2 score for GPR = ', gp_r2_value)\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model\n",
    "poly_mse_value = mean_squared_error(y_test, y_poly_pred)\n",
    "poly_r2_value = r2_score(y_test, y_poly_pred)\n",
    "print('MSE for polynomial = ', poly_mse_value)\n",
    "print('R2 score for polynomial = ', poly_r2_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly, the GPR approximation is much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Redo the Gaussian Process approximation but using the Matern kernel\n",
    "\n",
    "Of course, the **choice of kernel** used in Gaussian Process Regression (GPR) affects the quality of the prediction...\n",
    "\n",
    "* Let's go back to the code where we trained the Gaussian process, but now let's use a different kernel: the Matern kernel. Re-run the cells and compare the error metrics.\n",
    "\n",
    "Probably you found that the GPR prediction is still much better than the polynomial approximation, but not as good as the approximation obtained with the RBF kernel.\n",
    "\n",
    "* Can you hypothesize why that happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Process regression for noisy datasets\n",
    "\n",
    "Let's recreate the noisy dataset from $f(x)=x\\sin{x}$, as we did in Lecture 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's also create the noisy dataset:\n",
    "random_std = 0.5 + 1.0 * np.random.random(y_data.shape) # np.random.random returns random number between [0.0, 1.0)\n",
    "noise = np.random.normal(0, random_std) # sample vector from Gaussians with random standard deviation\n",
    "y_noisy_data = y_data + noise # Perturb every y_data point with Gaussian noise\n",
    "\n",
    "# Pair up points with their associated noise level (because of train_test_split):\n",
    "Y_noisy_data = np.column_stack((y_noisy_data,noise))\n",
    "\n",
    "# Split into 10% training points and the rest for testing:\n",
    "X_train, X_test, Y_noisy_train, Y_noisy_test = train_test_split(X_data,\n",
    "                                    Y_noisy_data, test_size=testset_ratio,\n",
    "                                    random_state=seed) # \"noisy_train\" is a great name for a variable, hein?\n",
    "# NOTE: since we are using the same seed and we do train_test_split on the same X_data and y_noisy_data is\n",
    "#       just y_data + noise, we are splitting the dataset exactly in the same way! This is nice because we\n",
    "#       want to keep the comparison as fair as possible.\n",
    "\n",
    "# Finally, for plotting purposes, let's convert the 2D arrays into 1D arrays (vectors):\n",
    "x_train = X_train.ravel()\n",
    "x_test = X_test.ravel()\n",
    "y_noisy_train = Y_noisy_train[:,0]\n",
    "noise_train = Y_noisy_train[:,1]\n",
    "y_noisy_test = Y_noisy_test[:,0]\n",
    "noise_test = Y_noisy_test[:,1]\n",
    "\n",
    "print(\"Note that X_train and X_test are the same data that we used for the noiseless case.\")\n",
    "print(\"Here's a print of X_train:\\n\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a Gaussian Process model\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "# Fitting for noisy data, if we have access to the uncertainty at the training points (usually we don't!), then\n",
    "# we can include the noise level at the alpha parameter\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=noise_train**2, n_restarts_optimizer=5)\n",
    "\n",
    "# Fit to data to determine the parameters of the model\n",
    "gp_model.fit(X_train, y_noisy_train)\n",
    "\n",
    "# Make the predictions\n",
    "y_noisy_pred, sigma_noisy = gp_model.predict(X_test, return_std=True) # predictions including uncertainty (std)\n",
    "y_noisy_data_pred, sigma_noisy_data = gp_model.predict(X_data, return_std=True) # for plotting\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig3, ax3 = plt.subplots() # This opens a new figure\n",
    "\n",
    "ax3.plot(x_data, f(x_data), 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # function to learn\n",
    "ax3.errorbar(x_train, y_noisy_train, noise_train, fmt='ro', markersize=6, label=u'training points inc. uncertainty')\n",
    "ax3.errorbar(x_test, y_noisy_test, noise_test, fmt='kX', markersize=6, label=u'testing points inc. uncertainty')\n",
    "\n",
    "ax3.plot(x_data, y_noisy_data_pred, 'b-', label=\"GPR prediction\")\n",
    "ax3.fill(np.concatenate([x_data, x_data[::-1]]),\n",
    "         np.concatenate([y_noisy_data_pred - 1.9600 * sigma_noisy_data,\n",
    "                        (y_noisy_data_pred + 1.9600 * sigma_noisy_data)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "ax3.set_xlabel('$x$', fontsize=20)\n",
    "ax3.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax3.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax3.legend(loc='upper left', fontsize=15)\n",
    "fig3.set_size_inches(8,8)\n",
    "plt.close(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig3 # plot figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Fit a polynomial of degree 4 (like we did last class) and compute the error metrics for that model as well as the above mentioned Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise.\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well done...\n",
    "\n",
    "Consider playing a bit with the notebook, using higher order degrees for the polynomial approximation and different number of training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Short tutorial on 1D regression with Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) have a lot more hyperparameters than Gaussian Processes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"docs/nn_basic_arch.png\" title=\"Simple ANN\" width=\"50%\" align=\"right\">\n",
    "\n",
    "Each circle represents a node (called neurons).\n",
    "\n",
    "Input layer: number of neurons equals number of features of our data\n",
    "\n",
    "Each connection of a hidden layer: $ h(x) = f(W_1x+b_1) $\n",
    "\n",
    "Output layer: $ o(x) = g(W_2h(x) + b_2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"docs/nn_manylayers.png\" title=\"ANN with two hidden layers\" width=\"50%\" align=\"right\">\n",
    "\n",
    "This is called a multilayer perceptron or fully-connected feed-forward neural network.\n",
    "\n",
    "* Neurons in hidden layers usually have the same non-linear function (ReLu is popular), weights are different for every neuron.\n",
    "\n",
    "* Many layers $\\rightarrow$ “deep learning”.\n",
    "\n",
    "* In theory: more hidden layers $\\rightarrow$ more complex functions and feature representation. But there's more to this story...\n",
    "\n",
    "* For regression each output neuron corresponds to a single output variable and the last layer uses a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will look into this model carefully in a few lectures.\n",
    "\n",
    "For now, I just want to draw a schematic so that you understand the number of parameters that starts appearing!\n",
    "\n",
    "* Draw on the board a feedforward ANN with 2 hidden layers for 1D case.\n",
    "    * First hidden layer with 3 neurons and second hidden layer with 2 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two examples of these nonlinear functions.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left\"; src=docs/nonlin_fn.png width=500px></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ReLu is the most common choice nowadays (easier optimization)\n",
    "\n",
    "* tanh has small gradients in most places (harder to optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick overview about neural networks\n",
    "\n",
    "* Non-linear regression models\n",
    "\n",
    "* Powerful for very large datasets\n",
    "\n",
    "* Non-convex optimization\n",
    "\n",
    "* Notoriously slow to train (state of the art models take days or even weeks to train, often on multiple GPUs)\n",
    "\n",
    "* Important to scale and transform the data properly (preprocessing)\n",
    "\n",
    "* MANY variants (Convolutional nets, Recurrent neural networks, variational autoencoders, generative adversarial networks, deep reinforcement learning, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Objective\n",
    "\n",
    "$ h(x) = f(W_1x+b_1) $\n",
    "$ o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)$\n",
    "\n",
    "$ \\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,o(x_i)) $\n",
    "\n",
    "$ =\\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,g(W_2f(W_1x+b_1)+b_2)$\n",
    "\n",
    "- $l$ is the MSE (or squared loss function) for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Need $ \\frac{\\partial l(y, o)}{\\partial W_i} $ and $\\frac{\\partial l(y, o)}{\\partial b_i}$\n",
    "\n",
    "\n",
    "* Example for network with one hidden layer where $ \\text{net}(x) := W_1x + b_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"docs/backprop_eqn.png\" title=\"ANN with two hidden layers\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing W, b\n",
    "\n",
    "* Batch\n",
    "\n",
    "    $ W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=1}^N \\frac{\\partial l(x_j,y_j)}{\\partial W_i} $\n",
    "\n",
    "\n",
    "* Online/Stochastic\n",
    "\n",
    "    $ W_i \\leftarrow W_i - \\eta\\frac{\\partial l(x_j,y_j)}{\\partial W_i}$\n",
    "\n",
    "\n",
    "* Minibatch\n",
    "\n",
    "    $ W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=k}^{k+m} \\frac{\\partial l(x_j,y_j)}{\\partial W_i}$\n",
    "\n",
    "Below you will find some notes about different optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**A nice resource about optimizers** to understand what is learning rate (step size) and momentum: https://distill.pub/2017/momentum/\n",
    "\n",
    "\n",
    "To note:\n",
    "\n",
    "1. Standard gradient descent with constant learning rate (or step size) $\\eta$ is slow because we update a weight matrix $W_i$ using the old $W_i$ and taking a gradient step after summing over the whole training set.\n",
    "\n",
    "    * Using the entire training set for each forward pass through the network means that we need to make predictions for every point (without updating the weights) and then do a backward pass with backpropagation... One forward pass and one backward pass is what we call an epoch. So, if we do this, a single epoch has a lot of matrix multiplications to do a single gradient step.\n",
    "    \n",
    "    \n",
    "2. To speed this up we can do a stochastic approximation, i.e. stochastic gradient descent (or online gradient descent). Here, you pick a sample at random, compute the gradient just considering that sample, and then update the parameter. So you update the weights much more often, but you have a much less stable estimate of the gradient. In practice, we often just iterate through the data instead of picking a sample at random. And as with linear models, this is much faster than doing full batches for large datasets.\n",
    "\n",
    "    * Stochastic grandient descent is less stable (of course!).\n",
    "    \n",
    "    \n",
    "3. A compromise is to consider batch sizes of $k$ samples of the training set (also called mini-batches). For example, we could use 64 points per epoch (one forward and backward pass). In other words: we look at $k=64$ samples, compute the gradients, average them, and update the weights. That allows us to update much more often than looking at the whole dataset, while still having a more stable gradient. This strategy is easy to parallelize in modern CPUs and GPUs and it is very commonly used in practice. The reason why this is faster is basically that doing a matrix-matrix multiplication is faster than doing a bunch of matrix-vector operations.\n",
    "\n",
    "\n",
    "Finally, a short note: we could also be using smarter optimization methods, like second order methods or LBFGS, but these are often not very effective on these large non-convex problems. One, called levenberg-marquardt is actually a possibility, but it's not really used these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Heuristics\n",
    "\n",
    "* Constant learning rate $\\eta$ not good\n",
    "\n",
    "\n",
    "* Better: adaptive $\\eta$ for each entry of $W_i$ (large $\\eta$ in the beginning and small at the end)\n",
    "\n",
    "\n",
    "* Common approach: adam optimizer\n",
    "\n",
    "\n",
    "* There are many variants of optimizers... Remember: you will often get different solutions depending on how you pick the learning rate because you are solving a (very) non-convex problem. It's nearly impossible to actually find a global optimum. So nearly all of these strategies are heuristics, that have just proven well to work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Complexity Control\n",
    "\n",
    "These are the main ways to control complexity.\n",
    "\n",
    "* Decrease number of parameters\n",
    "\n",
    "\n",
    "* Regularization:\n",
    "\n",
    "    * L2 & L1 regularization (just like what you did [will do] in the Lab Assignment for Regularized Least Squares)\n",
    "    * Dropout: randomly prune neurons from the network\n",
    "\n",
    "\n",
    "* Early Stopping:\n",
    "\n",
    "    * Early stopping means that you compute the loss from a validation set and then you stop when you start to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now focus on how to create an ANN model for 1D regression using the following hyperparameters:\n",
    "1. A feedforward architecture with 2 dense hidden layers\n",
    "2. The ReLu activation function\n",
    "3. Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a function defining our Artificial Neural Network.\n",
    "from tensorflow import keras # fast library for ANNs\n",
    "from tensorflow.keras.optimizers import Adam # import the optimizer you want to use to calculate the parameters\n",
    "from keras.models import Sequential # to create a feedforward neural network\n",
    "from keras.layers.core import Dense # to create a feedforward neural network with dense layers\n",
    "#\n",
    "# Function to create the ANN model (in this case we are creating )\n",
    "def create_ANN(input_dimensions=1, # number of input variables\n",
    "               neurons1=3, # number of neurons in first hidden layer\n",
    "               neurons2=2, # number of neurons in second hidden layer\n",
    "               activation='relu', # activation function\n",
    "               optimizer='adam'): # optimization algorithm to compute the weights and biases\n",
    "    # create model\n",
    "    model = Sequential() # Feedforward architecture\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    model.add(Dense(1)) # output layer with just one neuron because we have only one output (1D problem!)\n",
    "    model.compile(loss='mse', # error metric to measure our NLL (loss)\n",
    "                  optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition, let's introduce something important: dataset preprocessing.\n",
    "\n",
    "Standardizing our dataset is good practice and can be important for many ML algorithms (ANNs included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Standardizing your dataset is good practice and can be important for ANNs!\n",
    "from sklearn.preprocessing import StandardScaler # standardize the dataset with scikit-learn\n",
    "#\n",
    "scaler = StandardScaler().fit(X_train) # Check scikit-learn to see what this does!\n",
    "#\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "X_data_scaled=scaler.transform(X_data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor # a new version will use scikeras\n",
    "# Now create your first ANN model!\n",
    "neurons1=200 # number of neurons for the first hidden layer\n",
    "neurons2=10 # number of neurons for the second hidden layer\n",
    "batch_size = len(X_train) # considering the entire dataset for updating the weights and biases in each epoch\n",
    "optimizer = Adam(learning_rate=0.001) # specifying the learning rate value for the optimizer (PLAY WITH THIS!)\n",
    "ANN_model = KerasRegressor(build_fn=create_ANN, neurons1=neurons1, neurons2=neurons2,\n",
    "                           batch_size=batch_size, epochs=150, optimizer=optimizer,\n",
    "                           validation_data=(scaler.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now that we created our first ANN model, let's fit it to our (scaled) dataset!\n",
    "history = ANN_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig_ANN, (ax1_ANN, ax2_ANN) = plt.subplots(1,2)\n",
    "# Create a plot for the loss history\n",
    "ax1_ANN.plot(history.history['loss']) # plot training loss\n",
    "ax1_ANN.plot(history.history['val_loss']) # plot testing loss\n",
    "ax1_ANN.set_title('Training and testing loss', fontsize=20)\n",
    "ax1_ANN.set_ylabel('loss', fontsize=20)\n",
    "ax1_ANN.set_xlabel('epoch', fontsize=20)\n",
    "ax1_ANN.legend(['training', 'testing'], loc='upper right', fontsize=15)\n",
    "\n",
    "# Create a plot for the ANN prediction\n",
    "ax2_ANN.plot(x_data, f(x_data), 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # show ground truth function\n",
    "ax2_ANN.plot(x_train, y_train, 'ro', markersize=6, label=\"training points\") # show training data\n",
    "ax2_ANN.plot(x_test, y_test, 'kX', markersize=6, label=\"testing points\") # show testing data\n",
    "\n",
    "y_pred = history.model.predict(X_data_scaled) # predict all data points with ANN\n",
    "\n",
    "ax2_ANN.plot(x_data, y_pred, 'b-', label=\"Neural Network prediction\") # plot prediction\n",
    "ax2_ANN.set_title(r'NN with '+str(neurons1)+' neurons in the 1st hidden layer, and '+str(neurons2)+' in the 2nd',\n",
    "                 fontsize=20)\n",
    "ax2_ANN.set_xlabel('$x$', fontsize=20)\n",
    "ax2_ANN.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax2_ANN.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "# Create figure with specified size\n",
    "fig_ANN.set_size_inches(16, 8)\n",
    "plt.close(fig_ANN) # do not plot the figure now. We will show it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig_ANN # show figure now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not the most amazing model you have ever seen, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Try again but now using 200 neurons for the first hidden layer and 10 for the second hidden layer.\n",
    "    - spoiler alert: a bit better, but far from amazing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's possible to try to find better hyperparameters (and ANNs have many!).\n",
    "\n",
    "The notes below (not shown in presentation) contain a simple code to do this by \"brute force\" in a procedure called grid search.\n",
    "\n",
    "But there are much better ways to find better hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping # a strategy for complexity control\n",
    "from sklearn.model_selection import GridSearchCV # simple (brute force) approach to find better hyperparameters.\n",
    "#\n",
    "# Function to create the ANN model\n",
    "#(I am writing the function again, in case we want to change some hyperparameters, e.g. use more layers)\n",
    "def create_ANN(input_dimensions=1,neurons1=10,neurons2=10,neurons3=10,neurons4=10,\n",
    "                 activation='relu',optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    model.add(Dense(neurons3, activation=activation)) # UNCOMMEND If you want a third hidden layer\n",
    "    model.add(Dense(neurons4, activation=activation)) # UNCOMMENT if you want a fourth hidden layer, etc.\n",
    "    model.add(Dense(1)) # output layer with just one neuron (we only have one output)\n",
    "    model.compile(loss='mse', optimizer=optimizer) # choose error metric and optimizer.\n",
    "    return model\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# create model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=30, mode='min')\n",
    "\n",
    "# define the grid search parameters:\n",
    "neurons1 = [5,20,200] # number of neurons in hidden layer 1 (it's a vector because we will run the model)\n",
    "#neurons1 = [5] # number of neurons in hidden layer 1\n",
    "#neurons2 = [5,10] # number of neurons in hidden layer 2 (if present; uncomment in create_ANN function)\n",
    "neurons2 = [5] # number of neurons in hidden layer 2 (if present; uncomment in create_ANN function)\n",
    "neurons3 = [10] # number of neurons in hidden layer 3 (if present; uncomment in create_ANN function)\n",
    "neurons4 = [10] # number of neurons in hidden layer 4 (if present; uncomment in create_ANN function)\n",
    "#\n",
    "batch_size = [len(X_train)] # here considering batch size as large as the training data.\n",
    "#\n",
    "epochs = [1000]\n",
    "#\n",
    "optimizer = ['adam'] # if we specify the optimizer as a string, then you use the default parameters\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#init_mode = ['uniform', 'lecun_uniform', 'normal', 'orthogonal', 'zero', 'one', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']    \n",
    "#\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs,neurons1=neurons1,neurons2=neurons2,\n",
    "                  neurons3=neurons3,neurons4=neurons4, # comment this line if you don't want to use layer 3 and 4\n",
    "                  #init_mode=init_mode, # comment this line if you are not specifying the initialization mode\n",
    "                  optimizer=optimizer)\n",
    "NN_model = KerasRegressor(build_fn=create_ANN)\n",
    "grid = GridSearchCV(estimator=NN_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, y_train, callbacks=[early_stopping],\n",
    "                       validation_data=(scaler.transform(X_test), y_test))\n",
    "history = grid_result.best_estimator_.fit(X_train_scaled, y_train,callbacks=[early_stopping],\n",
    "                                          validation_data=(scaler.transform(X_test), y_test))\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig_ANN, (ax1_ANN, ax2_ANN) = plt.subplots(1,2)\n",
    "# Create a plot for the loss history\n",
    "ax1_ANN.plot(history.history['loss']) # plot training loss\n",
    "ax1_ANN.plot(history.history['val_loss']) # plot testing loss\n",
    "ax1_ANN.set_title('Training and testing loss', fontsize=20)\n",
    "ax1_ANN.set_ylabel('loss', fontsize=20)\n",
    "ax1_ANN.set_xlabel('epoch', fontsize=20)\n",
    "ax1_ANN.legend(['training', 'testing'], loc='upper right', fontsize=15)\n",
    "\n",
    "# Create a plot for the ANN prediction\n",
    "ax2_ANN.plot(x_data, f(x_data), 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # show ground truth function\n",
    "ax2_ANN.plot(x_train, y_train, 'ro', markersize=6, label=\"training points\") # show training data\n",
    "ax2_ANN.plot(x_test, y_test, 'kX', markersize=6, label=\"testing points\") # show testing data\n",
    "\n",
    "y_pred = history.model.predict(X_data_scaled) # predict all data points with ANN\n",
    "\n",
    "ax2_ANN.plot(x_data, y_pred, 'b-', label=\"Neural Network prediction\") # plot prediction\n",
    "ax2_ANN.set_title(r'NN with '+str(neurons1)+' neurons in the 1st hidden layer, and '+str(neurons2)+' in the 2nd',\n",
    "                 fontsize=20)\n",
    "ax2_ANN.set_xlabel('$x$', fontsize=20)\n",
    "ax2_ANN.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax2_ANN.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "# Create figure with specified size\n",
    "fig_ANN.set_size_inches(16, 8)\n",
    "plt.close(fig_ANN) # do not plot the figure now. We will show it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig_ANN # show figure now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This neural network approximation is also not brilliant... Were you expecting this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='red'>HOMEWORK</font>\n",
    "\n",
    "Redo the neural network regression but now for the noisy dataset (use the same network we used for the noiseless dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# HOMEWORK.\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Solution to Exercise 1\n",
    "\n",
    "``` python\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "poly_model.fit(X_train,y_noisy_train) # fit the polynomial to our 5 points in X_train which is a 2D array\n",
    "y_poly_noisy_pred = poly_model.predict(X_test) # prediction of our polynomial\n",
    "\n",
    "# Plot the function and the polynomial prediction\n",
    "fig_ex1, ax_ex1 = plt.subplots()\n",
    "\n",
    "ax_ex1.plot(x_data, f(x_data), 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # function to learn\n",
    "ax_ex1.errorbar(x_train, y_noisy_train, noise_train, fmt='ro', markersize=6, label=u'training points inc. uncertainty')\n",
    "ax_ex1.errorbar(x_test, y_noisy_test, noise_test, fmt='kX', markersize=6, label=u'testing points inc. uncertainty')\n",
    "\n",
    "y_poly_noisy_data_pred = poly_model.predict(X_data) # prediction of our polynomial for all data points\n",
    "ax_ex1.plot(x_data, y_poly_noisy_data_pred, 'b-', label=\"Polynomial prediction\")\n",
    "\n",
    "ax_ex1.set_xlabel('$x$', fontsize=20)\n",
    "ax_ex1.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax_ex1.set_title(\"Polynomial approximation\", fontsize=20)\n",
    "ax_ex1.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax_ex1.legend(loc='upper left', fontsize=15)\n",
    "fig_ex1.set_size_inches(8,8)\n",
    "\n",
    "# Compute MSE and R2 for the GP model\n",
    "# NOTE: here we will compare with the noiseless function (in practice we don't have this information!).\n",
    "gp_mse_value = mean_squared_error(y_test, y_noisy_pred)\n",
    "gp_r2_value = r2_score(y_test, y_noisy_pred)\n",
    "print('MSE for GPR = ', gp_mse_value)\n",
    "print('R2 score for GPR = ', gp_r2_value)\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model\n",
    "poly_mse_value = mean_squared_error(y_test, y_poly_noisy_pred)\n",
    "poly_r2_value = r2_score(y_test, y_poly_noisy_pred)\n",
    "print('MSE for polynomial = ', poly_mse_value)\n",
    "print('R2 score for polynomial = ', poly_r2_value)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Solution to Homework of this Lecture\n",
    "``` python\n",
    "# Function to create the ANN model (a feedforward architecture with 2 hidden layers)\n",
    "def create_ANN(input_dimensions=1, # number of input variables\n",
    "               neurons1=3, # number of neurons in first hidden layer\n",
    "               neurons2=2, # number of neurons in second hidden layer\n",
    "               activation='relu', # activation function\n",
    "               optimizer='adam'): # optimization algorithm to compute the weights and biases\n",
    "    # create model\n",
    "    model = Sequential() # Feedforward architecture\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    model.add(Dense(1)) # output layer with just one neuron because we have only one output (1D problem!)\n",
    "    model.compile(loss='mse', # error metric to measure our NLL (loss)\n",
    "                  optimizer=optimizer)\n",
    "    return model\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# Standardizing your dataset is usually a good practice and can be important for ANNs:\n",
    "scaler = StandardScaler().fit(X_train) # Check scikit-learn to see what this does!\n",
    "#\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "X_data_scaled=scaler.transform(X_data)\n",
    "#\n",
    "\n",
    "# Now create your first ANN model!\n",
    "neurons1=200 # number of neurons for the first hidden layer\n",
    "neurons2=10 # number of neurons for the second hidden layer\n",
    "batch_size = len(X_train) # considering the entire dataset for updating the weights and biases in each epoch\n",
    "ANN_model = KerasRegressor(build_fn=create_ANN, neurons1=neurons1, neurons2=neurons2,\n",
    "                           batch_size=batch_size, epochs=150, optimizer='adam',\n",
    "                           validation_data=(scaler.transform(X_test), y_noisy_test))\n",
    "\n",
    "# Now let's train the model on our (scaled) dataset!\n",
    "history = ANN_model.fit(X_train_scaled, y_noisy_train)\n",
    "\n",
    "# Finally, plot the loss history and the predicted function.\n",
    "fig_ANN, (ax1_ANN, ax2_ANN) = plt.subplots(1,2)\n",
    "# Create a plot for the loss history\n",
    "ax1_ANN.plot(history.history['loss']) # plot training loss\n",
    "ax1_ANN.plot(history.history['val_loss']) # plot testing loss\n",
    "ax1_ANN.set_title('Training and testing loss', fontsize=20)\n",
    "ax1_ANN.set_ylabel('loss', fontsize=20)\n",
    "ax1_ANN.set_xlabel('epoch', fontsize=20)\n",
    "ax1_ANN.legend(['training', 'testing'], loc='upper right', fontsize=15)\n",
    "\n",
    "# Create a plot for the ANN prediction\n",
    "ax2_ANN.plot(x_data, f(x_data), 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # show ground truth function\n",
    "ax2_ANN.errorbar(x_train, y_noisy_train, noise_train, fmt='ro', markersize=6, label=u'training points inc. uncertainty')\n",
    "ax2_ANN.errorbar(x_test, y_noisy_test, noise_test, fmt='kX', markersize=6, label=u'testing points inc. uncertainty')\n",
    "\n",
    "y_pred = history.model.predict(X_data_scaled) # predict all data points with ANN\n",
    "\n",
    "ax2_ANN.plot(x_data, y_pred, 'b-', label=\"Neural Network prediction\") # plot prediction\n",
    "ax2_ANN.set_title(r'NN with '+str(neurons1)+' neurons in the 1st hidden layer, and '+str(neurons2)+' in the 2nd',\n",
    "                 fontsize=20)\n",
    "ax2_ANN.set_xlabel('$x$', fontsize=20)\n",
    "ax2_ANN.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax2_ANN.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "# Create figure with specified size\n",
    "fig_ANN.set_size_inches(16, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### End of Practical Session 1\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm] *",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
