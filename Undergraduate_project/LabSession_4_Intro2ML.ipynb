{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3i0H-lcTIGMM"
   },
   "source": [
    "# WS2332 - Project 7 - Lab Session 4: **Bayesian Optimization**\n",
    "## Step 0: This is how it looks like\n",
    "\n",
    "Here you can see Bayesian optimization in action for a (1D, noiseless) problem (gif source: Wikipedia). \n",
    "\n",
    "A quick explanation about what the most important things mean in each step of this animation:\n",
    "\n",
    "In the top figure:\n",
    "*   The **black line** represents the ground truth function $f(x)$.\n",
    "*   The **purple line** represents the predictive mean $\\mu_*$.\n",
    "*   The **purple band** represents the 95% confidence interval; its width is twice the predictive standard deviation $\\sigma_*$.\n",
    "\n",
    "In the bottom three figures:\n",
    "*   The **cyan line** represents a specific acquisition function $\\text{acq}(x)$. In these cases: EI, UCB and PI respectively.\n",
    "*   The **yellow diamond** represents the maximum value of the acquisition function. The argument at which this happens is the predictive / test point $x_*$ for the next step of the iteration.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRRTdv3eFlAM"
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCzTHoVZW8t4"
   },
   "source": [
    "---\n",
    "## Step 1: Install GPyOpt\n",
    "\n",
    "This following command will install [GPyOpt](https://sheffieldml.github.io/GPyOpt/). Be sure to do this before you proceed to the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxK-qsxkErgh"
   },
   "outputs": [],
   "source": [
    "pip install GPyOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glESZv8EG0cE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Step 2: Simple problem (1D)\n",
    "\n",
    "Let's first import the Python packages that are needed to do run some basic tasks. You only need to do this once per session. As a recap:\n",
    "\n",
    "*   [NumPy](https://numpy.org/) is a package that allows us a lot of freedom to do mathematical manipulations. This includes standard functions such as sine, cosine and exponential functions, but also (pseudo-)random number generation with or without a given seed.\n",
    "*   [SciPy](https://docs.scipy.org/doc/scipy/reference/) is a package containing additional routines such as common function optimization algorithms.\n",
    "*   [Matplotlib](https://matplotlib.org/) is a package that governs visualizing objects Python. We use it to plot our graphs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYw4VOn8cFhz"
   },
   "outputs": [],
   "source": [
    "# --- Load GPyOpt + other packages\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "import numpy as np\n",
    "import scipy.optimize as so\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOrQ2uu4ctuZ"
   },
   "source": [
    "Now we're ready to tackle our first problem. Find the minimum value of $f(x)=x\\sin(x)$ on the interval $I=[-3, 3]$. \n",
    "\n",
    "This should be $\\min_{x\\in I}f(x)=0$, attained at $x=0$.\n",
    "\n",
    "We will also compare the overall approximation of the function with the true function, and compare the performance of BO versus another common derivative-free optimization algorithm called [Nelder-Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4Vx_H4NFNCW"
   },
   "outputs": [],
   "source": [
    "# --- Fixed seed for consistent results\n",
    "np.random.seed(123)\n",
    "\n",
    "# --- Define your problem\n",
    "def f(x): return x * np.sin(x)\n",
    "domain = [{'type': 'continuous', 'domain': (-3, 3)}]\n",
    "\n",
    "# --- Solve your problem\n",
    "numberOfInitialPoints = 1\n",
    "numberOfIterations = 9\n",
    "\n",
    "myBopt = BayesianOptimization(f=f, acquisition_type='EI', domain=domain, normalize_Y=False, initial_design_numdata=numberOfInitialPoints)\n",
    "myBopt.run_optimization(max_iter=numberOfIterations)\n",
    "myBopt.plot_acquisition()\n",
    "\n",
    "# --- Comparing the ground truth with the approximated function\n",
    "xgrid = np.linspace(-3, 3, 100).reshape((100, 1))\n",
    "plt.plot(xgrid, myBopt.model.model.predict(xgrid)[0], ':', label='Predicted function')\n",
    "plt.plot(xgrid, f(xgrid), label='Ground truth')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Printing some interesting stuff\n",
    "print(\"Next acquisition minimizing argument: \")\n",
    "print(myBopt.suggest_next_locations())\n",
    "print()\n",
    "\n",
    "print(\"Progression of the approximated minimum function value:\")\n",
    "print(myBopt.Y_best.reshape((len(myBopt.Y_best), 1)))\n",
    "print()\n",
    "\n",
    "# --- Compare with Nelder-Mead optimization\n",
    "NMmaxiter = int((numberOfInitialPoints + numberOfIterations) / 2)\n",
    "nmopt = so.minimize(fun=f, x0=1, method='Nelder-Mead', bounds=(-3, 3), options={'maxiter': NMmaxiter})\n",
    "print('N-M optimized value after', 2 * NMmaxiter, 'function evaluations:', nmopt.fun)\n",
    "# print(nmopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7YytHopr1I0"
   },
   "source": [
    "# Step 3: Not-so simple problem (1D)\n",
    "\n",
    "We are now ready to look at a function that has a more irregular landscape. Let's consider the **Ackley function**, which is defined as\n",
    "$$A(x)=20(1-e^{-|x|/5})-e^{\\cos(2\\pi x)}+e.$$\n",
    "\n",
    "This function has a unique global minimum is located at $x=0$ with $A(0)=0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20DtoYHBYR9O"
   },
   "outputs": [],
   "source": [
    "# --- Your code here to define the Ackley function and to plot it (optional). Suggested: between -15 and 15 with 300 grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Reh3SFPlWfT"
   },
   "outputs": [],
   "source": [
    "# --- Fixed seed for consistent results\n",
    "np.random.seed(123)\n",
    "\n",
    "# -- Your code here to run Bayesian optimization to find the minimum A(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cs9XgP3OWo58"
   },
   "source": [
    "# Step 4: 2D problem\n",
    "\n",
    "Let's have a look at optimizing a function with two continuous variables. In particular, we will analyze how BO handles a notoriously difficult function to optimize: the **Rosenbrock function**, defined as\n",
    "$$R(x,y)=(1-x)^2+100(y-x^2)^2.$$\n",
    "This function also has a unique global minimum located at $(x,y)=(1,1)$ with value $R(1,1)=0$.\n",
    "\n",
    "The Nelder-Mead optimization process for the Rosenbrock function (gif source: Wikipedia):\n",
    "\n",
    "![](https://i.imgur.com/V6EF7Sm.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqyYZHaSmhQy"
   },
   "outputs": [],
   "source": [
    "# --- Fixed seed for consistent results\n",
    "np.random.seed(123)\n",
    "\n",
    "# --- Define your problem\n",
    "def Rbase(x, y): return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
    "def R(x): return Rbase(x[:, 0], x[:, 1])\n",
    "\n",
    "def RNM(x): return Rbase(x[0], x[1])\n",
    "domainR = [{'name': 'x', 'type': 'continuous', 'domain': (0, 2)},\n",
    "           {'name': 'y', 'type': 'continuous', 'domain': (0, 2)}]\n",
    "            \n",
    "# --- Solve your problem\n",
    "numberOfInitialPoints = 5\n",
    "numberOfIterations = 35\n",
    "\n",
    "myBoptR = BayesianOptimization(f=R, acquisition_type='EI', domain=domainR, normalize_Y=True, initial_design_numdata=numberOfInitialPoints)\n",
    "myBoptR.run_optimization(max_iter=numberOfIterations)\n",
    "myBoptR.plot_acquisition()\n",
    "\n",
    "# --- Compare with Nelder-Mead optimization\n",
    "bounds = so.Bounds([0, 0], [2, 2])\n",
    "x0 = np.array([0.5, 0.5])\n",
    "\n",
    "NMmaxiter = int((numberOfInitialPoints + numberOfIterations) / 2)\n",
    "nmopt = so.minimize(fun=RNM, x0=x0, method='Nelder-Mead', bounds=bounds, options={'maxiter': NMmaxiter})\n",
    "print('N-M optimized value after', 2 * NMmaxiter, 'function evaluations:', nmopt.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTlB3jg6sKSM"
   },
   "outputs": [],
   "source": [
    "print ('BAYESIAN OPTIMIZATION')\n",
    "print ( '{0:4s}   {1:9s}   {2:9s}      {3:9s}'.format('Iter', ' X1', ' X2', 'f(X)')   )\n",
    "for i in range(numberOfInitialPoints+numberOfIterations):\n",
    "    print('{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}'.format(i+1, myBoptR.X[i,0], myBoptR.X[i,1], myBoptR.Y[i,0]))\n",
    "print('Bayesian optimized value after', numberOfInitialPoints+numberOfIterations, 'function evaluations:', myBoptR.Y_best[-1])\n",
    "\n",
    "\n",
    "print ()\n",
    "\n",
    "# --- Compare with Nelder-Mead optimization\n",
    "\n",
    "\n",
    "bounds = so.Bounds([0, 0], [2, 2])\n",
    "x0 = np.array([0.5, 0.5])\n",
    "xxi = [np.array([x0[0],x0[1],RNM(x0)])]\n",
    "\n",
    "\n",
    "Nfeval = 1\n",
    "\n",
    "\n",
    "print ('NELDER-MEAD')\n",
    "print ( '{0:4s}   {1:9s}   {2:9s}      {3:9s}'.format('Iter', ' X1', ' X2', 'f(X)')   )\n",
    "\n",
    "print ('{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}'.format(Nfeval, x0[0], x0[1], RNM(x0)))\n",
    "\n",
    "def callbackF(Xi):\n",
    "    xxi.append(np.array([Xi[0],Xi[1],RNM(Xi)]))\n",
    "    global Nfeval\n",
    "    Nfeval += 1\n",
    "    print('{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}'.format(Nfeval, Xi[0], Xi[1], RNM(Xi)))\n",
    "\n",
    "\n",
    "NMmaxiter = int((numberOfInitialPoints + numberOfIterations) / 2)\n",
    "nmopt = so.minimize(fun=RNM, x0=x0, callback=callbackF, method='Nelder-Mead', bounds=bounds, options={'maxiter': NMmaxiter})\n",
    "print('N-M optimized value after', 2 * NMmaxiter, 'function evaluations:', nmopt.fun)\n",
    "# print(nmopt)\n",
    "\n",
    "\n",
    "xxi = np.array(xxi)\n",
    "\n",
    "\n",
    "# ---- Plotting #\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(myBoptR.Y_best, label='BO')\n",
    "plt.plot(xxi[:,-1], label='N-M')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Best objective function value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0chikL0oXoD7"
   },
   "source": [
    "# Step 5: higher-dimensional problems\n",
    "\n",
    "Let's move on to higher dimensional problems. The **$d$-dimensional Ackley function** is given by\n",
    "$$A(x_1,x_2,\\ldots,x_n)=20\\left(1-\\exp\\left(-\\frac15\\sqrt{\\frac1d\\sum_{i=1}^dx_i^2}\\right)\\right)-\\exp\\left(\\frac1d\\sum_{i=1}^d\\cos(2\\pi x_i)\\right)+e.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itEhlh5TbDb1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def Ad(x, d):\n",
    "  return 20 * (1 - np.exp(-np.sqrt(np.sum(x ** 2, 1) / d) / 5)) - np.exp(np.sum(np.cos(2 * np.pi * x), 1) / d) + np.e\n",
    "\n",
    "xgrid = np.linspace(-15, 15, 300)\n",
    "ygrid = np.linspace(-15, 15, 300)\n",
    "X, Y = np.meshgrid(xgrid, ygrid)\n",
    "inpoints = np.array([X, Y]).reshape(2, -1).T\n",
    "outpoints = np.reshape(Ad(inpoints, 2), np.shape(X))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, outpoints, cmap=cm.cool,\n",
    "                       linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2InIb47b_Ce"
   },
   "outputs": [],
   "source": [
    "import GPyOpt\n",
    "# --- Fixed seed for consistent results\n",
    "np.random.seed(123)\n",
    "\n",
    "# --- Define your problem\n",
    "funcAd = GPyOpt.objective_examples.experimentsNd.ackley(input_dim=2) \n",
    "domainAd = [{'type': 'continuous', 'domain': (-15, 15)},\n",
    "            {'type': 'continuous', 'domain': (-15, 15)}]\n",
    "\n",
    "# --- Solve your problem\n",
    "numberOfInitialPoints = 5\n",
    "numberOfIterations = 25\n",
    "\n",
    "myBoptAd = BayesianOptimization(f=funcAd.f, acquisition_type='EI', domain=domainAd, normalize_Y=False, initial_design_numdata=numberOfInitialPoints)\n",
    "myBoptAd.run_optimization(max_iter=numberOfIterations)\n",
    "myBoptAd.plot_convergence()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jVF_LoUTsDsD"
   ],
   "name": "WS2332 - Project 7 - Lab Session 4: Bayesian Optimization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
